\graphicspath{{chapters/chp1/graphics/}}
\title{cuDOLFINx: A CUDA Extension for FEniCSx}
\titlerunning{cuDOLFINx: A CUDA Extension for FEniCSx}

\author{Benjamin~A.~Pachev, James~D.~Trotter, and Igor~A.~Baratta}
\authorrunning{Pachev et al.}
\institute{Benjamin.~A.~Pachev \email{benjmainpachev@utexas.edu} \at The University of Texas at Austin, Austin, Texas, United States \\James.~D.~Trotter \email{james@simula.no} \at Simula Research Laboratory, Oslo, Norway
\\Igor~A.~Baratta \email{ia397@cam.ac.uk} \at Department of Engineering, University of Cambridge, Cambridge, United Kingdom
}


\maketitle

\abstract{This chapter introduces cuDOLFINx, a Python package that extends FEniCSx with GPU-accelerated assembly capabilities.
The extension enables FEniCSx codes to be accelerated on the GPU with minimal changes and provides an easy way for researchers to experiment with GPU-accelerated partial differential equation solvers. By contrast with previous efforts to enhance FEniCSx with GPU capabilities, cuDOLFINx is designed as a standalone package and does not require major changes to the core components of FEniCSx. Consequently, it has the potential to become a usable part of the FEniCSx ecosystem and a long-term solution to the problem of providing GPU acceleration capabilities in FEniCSx.
We further present performance benchmarks for representative GPU-accelerated FEniCSx applications on an NVIDIA GH200 GPU.
Our results indicate that GPU-accelerated assembly routines within cuDOLFINx can be up to 40 times faster than traditional FEniCSx assembly with MPI parallelization on a multi-core CPU node.}

\section{Introduction}
Graphics processing units (GPUs) provide an alternative means of parallelizing computations compared to traditional clusters of multi-core CPUs. For many applications, GPUs are more energy efficient and have thus revolutionized fields such as machine learning~\citep{navarro2014survey}. Several well-known software packages used for solving partial differential equations (PDEs) have taken steps to provide GPU acceleration capabilities, including PETSc~\citep{MILLS2021102831}, MFEM~\citep{anderson2021mfem}, libCEED~\citep{abdelfattah2021gpu}, and deal.II~\citep{arndt2021deal}.
However, the GPU acceleration of PDE solvers has yet to become the norm, since other PDE libraries~\citep{baratta2023dolfinx,schoberl2014c++,hecht2012new,moxey2020nektar++,FiredrakeUserManual} lack support for GPU acceleration. Furthermore, even when GPU acceleration is available, it often has limited support and can be difficult to use. Modifying existing code to use GPU parallelism remains a significant challenge~\citep{MILLS2021102831}. GPU programming requires a specialized compiler, memory space, and syntax, such that code must often be largely rewritten to take advantage of GPU acceleration.


A major attraction of FEniCSx~\citep{baratta2023dolfinx} as a tool for solving PDEs with the finite element method is its simple Python interface and automated generation of efficient C code. These features enable the rapid prototyping and development of performant solvers for complicated PDEs. Our goal in developing cuDOLFINx \citep{cudolfinxzenodo} is to enable FEniCSx users to add GPU acceleration to their existing PDE solvers with minimal effort.
GPUs are attractive for PDEs due to their increased throughput, high number of floating-point operations per second (FLOPS), and memory bandwidth. Although many PDEs are generally memory bound, even memory-bound problems can benefit significantly from GPU acceleration. The remainder of this chapter will provide a brief overview of cuDOLFINx; present example applications for the Poisson, Navier--Stokes, and shallow water problems; and, finally, discuss the future development of cuDOLFINx.

\section{Overview of cuDOLFINx}
The two most expensive steps in the finite element method are linear solves and the assembly of matrices or vectors.
We would like to accelerate both of these steps with GPUs, partly to minimize expensive copies to and from GPU memory.
The PETSc~\citep{MILLS2021102831}, Ginkgo~\citep{ginkgo-toms-2022}, AMGx~\citep{naumov2015amgx}, hypre~\citep{li2020efficient,falgout2021porting}, SuperLU~\citep{li2023newly}, and other libraries~\citep{lu2023tilesptrsv} provide efficient GPU-accelerated linear solvers.
The goal of cuDOLFINx is to enable GPU-accelerated assembly so that the entirety of FEniCSx finite element workflows can be GPU accelerated.

FEniCSx relies on auto-generated kernels to perform element-wise numerical integration, and the resulting element matrices or vectors can be assembled to form global matrices or vectors.
In cuDOLFINx, these kernels are modified to execute on a GPU using CUDA.
The generated kernels from the FEniCSx Form Compiler (FFCx) are used as is, with no GPU-targeted changes.
Each element is processed by a single GPU thread, and atomic operations are used to prevent data races.
This approach works well for low-order elements but can be problematic for higher orders, since the computation of element stiffness matrices at high orders can require more local memory than is available to a single GPU thread. This can increase the usage of slower memory and reduce the number of GPU threads that can execute concurrently. Consequently, GPU assembly routines for high-order methods typically assign multiple threads to each element~\citep{MACIOL20101093,dziekonski2013generation,abdelfattah2021gpu,swirydowicz2019acceleration}.
This is a goal of future work and will require extending FFCx to support GPU parallelism within element kernels.

In addition to the element-wise kernels, cuDOLFINx provides GPU-based assembly loops to assemble the local contributions from each element into global matrices or vectors. This requires information about the mesh, boundary conditions, and function spaces to be copied to GPU memory. Internally, cuDOLFINx provides GPU counterparts for many of the data structures used in FEniCSx and automatically performs the data transfer to the GPU. Once the data are transferred, they are expected to remain on the GPU, though mechanisms exist for moving data between the CPU and GPU when necessary.

The cuDOLFINx package extends the work of \cite{trotter2023targeting}. In addition to major modifications needed to support DOLFINx 0.9 (the most recent version at the time of writing), the following new features have been developed:
\begin{enumerate}
    \item Support for boundary integrals was developed from scratch and considerably improved compared to the original version of the code, which only supported boundary integrals in very limited scenarios.
    \item To support discontinuous Galerkin (DG) methods, integrals on interior mesh edges are required. This functionality was added to the original GPU-accelerated code.
    \item Most FEniCSx users utilize the Python version of the library. Consequently, a Python API was developed for the GPU acceleration capabilities. It was designed to be much simpler to use than the original C++ interface, without loss of functionality.
    \item The GPU acceleration scheme was applied to a wider range of problems, including the Navier--Stokes and shallow water equations.
\end{enumerate}

There are three main cuDOLFINX software components.
The first is a set of C++ classes containing the CUDA data structures needed for assembly.
These classes correspond to their DOLFINx counterparts and are responsible for copying the required data to the GPU.
The second consists of C++ classes that manage the generation, runtime compilation, and launching of CUDA assembly kernels.
The final component contains the Python bindings for the C++ core. Most users will only need to utilize the Python wrapper, which provides Python versions of the C++ CUDA data structure classes, as well as convenience routines for performing assembly operations.
We demonstrate the Python wrapper's usage in the following section.
\section{Sample Usage}

Using cuDOLFINx within existing FEniCSx code requires only minor modifications. Consider the following example for solving Poisson's equation on the unit square:
\begin{python}
from mpi4py import MPI
from dolfinx import fem, mesh
import cudolfinx as cufem
from ufl import dx, inner, grad
import ufl

N = 1000
domain = mesh.create_unit_square(MPI.COMM_WORLD, N, N)
V = fem.functionspace(domain, ("Lagrange", 1))
f = fem.Function(V)
f.interpolate(lambda x: x[0]**2 + x[1])
u, v = ufl.TestFunction(V), ufl.TrialFunction(V)
A = -inner(grad(u), grad(v)) * dx
L = f * v * ufl.dx
\end{python}
Having defined a bilinear form \pythoninline{A} and a linear form \pythoninline{L}, the following code creates CUDA counterparts of each form and a \pythoninline{CUDAAssembler} to offload the assembly of the matrix and right-hand side vector to a CUDA-enabled GPU:
\begin{python}
cuda_A = cufem.form(A)
cuda_L = cufem.form(L)
asm = cufem.CUDAAssembler()
mat = asm.assemble_matrix(cuda_A)
vec = asm.assemble_vector(cuda_L)
\end{python}
The assembled matrix and vector are provided in the form of \pythoninline{CUDAMatrix} and \pythoninline{CUDAVector} types, which can be readily converted to the corresponding PETSc types for GPU-resident matrices and vectors. This approach transparently enables the use of PETSc's GPU-accelerated solver routines. 


Offloading matrix or vector assembly to a GPU typically results in faster assembly but it also incurs overhead due to copying necessary data structures to the device, as well as the runtime compilation of CUDA assembly kernels. For large enough problems, the overhead is small compared to the assembly itself. Moreover, the overhead is incurred only once per form and is therefore negligible for applications that require repeated assembly operations. Such use cases are very common and include time-dependent or nonlinear problems.

Below, we show how a simplified FEniCSx code for Newton iteration might be modified to support GPU acceleration.
Boundary conditions are excluded for brevity; however, complete working examples with boundary conditions are available in the cuDOLFINX source code \citep{cudolfinxzenodo}. Our example equation is a nonlinear version of Poisson's equation with an extra cubic term in $u$.
We begin by initializing the PETSc data structures needed for assembly and linear algebra.
\begin{python}
from petsc4py import PETSc
use_cuda = True
u = fem.Function(V)
u_update = fem.Function(V)
residual = (f * v - inner(grad(u), grad(v)) + u**3 * v) * dx
jacobian = ufl.derivative(residual, u)

if use_cuda:
  # Force DOLFINx to use a CUDA PETSc vector
  u.vector.setType(PETSc.Vec.Type.CUDA)
  u_update.vector.setType(PETSc.Vec.Type.CUDA)
  asm = cufem.CUDAAssembler()
  residual, jacobian = cufem.form(residual), cufem.form(jacobian)
  L = asm.create_vector(residual)
  A = asm.create_matrix(jacobian)
else:
  residual, jacobian = fem.form(residual), fem.form(jacobian)
  L = petsc_fem.create_vector(residual)
  A = petsc_fem.create_matrix(jacobian)
\end{python}
The next step is to create the linear solver object. This requires a slight syntactical change when using cuDOLFINx, to extract the underlying PETSc matrix from the assembled \pythoninline{CUDAMatrix}.
\begin{python}
ksp = PETSc.KSP().create(domain.comm)
ksp.setType("gmres")
ksp.getPC().setType("jacobi")
if use_cuda:
  # Get underlying PETSc Mat, as A is a CUDAMatrix
  ksp.setOperators(A.mat)
else:
  ksp.setOperators(A)
\end{python}
Finally, we reach the Newton iteration loop. Note the use of PETSc operations to add the computed Newton update to the solution, instead of vectorized NumPy operations as is common in FEniCSx code.
This allows the computation to happen on the GPU and is the reason that \pythoninline{u_update.vector} and \pythoninline{u.vector} have to be PETSc CUDA vectors.
\begin{python}
for i in range(5):
  if use_cuda:
    # by default entries are zeroed prior to assembly
    asm.assemble_matrix(jacobian, mat=A)
    A.assemble()
    asm.assemble_vector(residual, L)
    rhs = L.vector
  else:
    A.zeroEntries()
    petsc_fem.assemble_matrix(A, jacobian)
    A.assemble()
    L.array[:] = 0
    petsc_fem.assemble_vector(L, residual)
    rhs = L

  rhs.scale(-1)
  ksp.solve(rhs, u_update.vector)
  u.vector.axpy(1.0, u_update.vector)
\end{python}
A final step remains within the loop. The updated solution needs to be copied back to the underlying DOLFINx function, which resides on the host. This is not required for regular PETSc vectors, which share the same memory with the function object, but it is a requirement for correctness in the case of CUDA-type vectors, which use a separate GPU memory space.
\begin{python}
  if use_cuda:
    # Ensure host-side values of u match device-side values
    u.x.array[:len(u.vector.array)] = u.vector.array
    u.x.scatter_forward()
\end{python}

We will show representative use cases in which GPU acceleration can significantly enhance FEniCSx performance.
\section
{Performance Evaluation}
We begin with two examples of the performance of the GPU assembly kernels in cuDOLFINx and then present a use case of complete GPU offloading for both assembly and linear solves.
All computations in the following experiments are performed in double precision.
The primary hardware used in the following experiments is an NVIDIA GH200 Superchip, which consists of a Hopper GPU with 132 streaming multiprocessors connected to a 72-core Grace CPU.
The Hopper GPU has a memory bandwidth of 4 TB/s, while the CPU has a memory bandwidth of 384 GB/s~\citep{gh200specs}.
Experiments were conducted using the Vista system at the Texas Advanced Computing Center.
\subsection{Poisson Equation}
We now consider the problem of assembling a stiffness matrix for the solution of the Poisson equation on the unit cube. We use linear Lagrange finite elements on four different uniform tetrahedral meshes ranging in size from about 1.3 million to 16.5 million elements.
For each mesh, \Cref{tab:poisson_results} reports the throughput in millions of degrees of freedom (DOFs) per second (MDOF/s) for assembling the stiffness matrix with cuDOLFINx on a Hopper GPU and with DOLFINx on a Grace CPU.

The GPU does not appear fully saturated for the first two meshes, both of which have significantly fewer than 1 million DOFs.
This suggests that problems with approximately 1 million DOFs or more are needed to maximize the benefits of GPU acceleration.
\cite{trotter2023targeting} report a throughput of 189\,MDOF/s for the same problem using the Uniform 3 mesh on an NVIDIA V100 GPU.
In this study, we achieved a throughput of 376 MDOF/s on the GH200 GPU, roughly twice that of the V100. This result is expected, since the GH200 offers higher memory bandwidth and FLOPS.
Further investigation will explore the underlying factors behind these performance gains.
\begin{table}[t]
    \centering
\begin{tabular}{lrrrr}
\toprule
          &          &             & \multicolumn{2}{l}{Matrix assembly} \\
                                     \cmidrule(lr){4-5}
Mesh      & Elements & DOFs        & Hopper (GPU) & Grace (CPU) \\
\midrule
Uniform 1 &  1,296,000 &   226,981 & 182.8 & 19.9 \\
Uniform 2 &  3,072,000 &   531,441 & 297.5 & 25.6 \\
Uniform 3 &  6,000,000 & 1,030,301 & 376.5 & 13.0 \\
Uniform 4 & 16,464,000 & 2,803,221 & 373.3 & 29.8 \\
\bottomrule
\end{tabular}
\caption{Performance of Poisson matrix assembly, in millions of DOFs per second.}
    \label{tab:poisson_results}
\end{table}
\subsection
{Shallow Water Equations}
For a more realistic example that also displays the complexity enabled by FEniCSx, we present assembly benchmarks using the variational forms in SWEMniCS~\citep{dawson2024swemnics}, a FEniCSx-based solver for the shallow water equations.
SWEMniCS implements a suite of stabilized 2D shallow water solvers with implicit time stepping.
Due to the nonlinearity of the shallow water equations, a Newton solver is required at each time step.

We investigate the efficiency of automatically generated CUDA assembly kernels for two stabilized schemes within SWEMniCS: the DG and the streamline upwind Petrov--Galerkin (SUPG) schemes. The DG method uses broken test and trial spaces with a Lax--Freidrichs numerical flux. The DG scheme is more numerically stable but requires more DOFs than the SUPG scheme due to the use of discontinuous function spaces. For each scheme, we averaged the performance of both GPU and CPU assembly kernels over 20 Newton iterations for tidal flow simulations on square domains with uniform triangular meshes. \Cref{tab:swe_a100_vs_epyc} shows the speedup when using an NVIDIA A100 GPU compared to a 64-core AMD EPYC 7763 CPU for assembling the Jacobian matrix and the residual vector for each scheme. The A100 GPU and EPYC CPU were both on the Lonestar6 system at the Texas Advanced Computing Center.
\begin{table}[t]
    \centering
    \begin{tabular}{lrrrrrrr}
\toprule
        &           &           \multicolumn{3}{c}{DG} & \multicolumn{3}{c}{SUPG} \\
                                  \cmidrule(lr){3-5}       \cmidrule(lr){6-8}
Mesh    &  Elements &  DOFs & Jacobian & Residual  & DOFs  & Jacobian & Residual \\
\midrule
Tidal 1 &   980,000 &  8,820,000 &     0.98 &      9.21 &  1,474,203 &   5.36 &     6.78 \\
Tidal 2 & 2,000,000 & 18,000,000 &     0.99 &     12.18 &  3,006,003 &   5.51 &     6.84 \\
Tidal 3 & 3,380,000 & 30,420,000 &     0.95 &      9.66 &  5,077,803 &   5.36 &     5.60 \\
\bottomrule
\end{tabular}
    \caption{Speedup of assembly kernels on an NVIDIA A100 GPU relative to a 64-core AMD EPYC CPU. Larger numbers indicate faster GPU runtimes.}
    \label{tab:swe_a100_vs_epyc}
\end{table}


Interestingly, the speedups differ for each variational form. In the case of the DG scheme, assembly of the residual vector obtains a speedup of 9--12${\times}$, whereas assembly of the Jacobian on the A100 GPU is comparable in performance to that of the 64-core AMD EPYC CPU. The SUPG scheme, on the other hand, obtains a consistent speedup of 5--7${\times}$ for both residual and Jacobian matrix assembly. 

We can use NVIDIA's Nsight Compute profiler to better understand the difference in performance between the DG and SUPG schemes. 
\Cref{tab:tidal_prof} presents the profiler results for the Tidal 3 test case. The profiler reports \textit{occupancy}, which indicates the percentage of active threads on the GPU relative to the total GPU thread capacity. Occupancy can be limited by the resources needed per thread, such as shared memory or registers. It can also be impacted by excessive branching or other kernel design flaws. In our case, the generated assembly kernels required a high number of registers, limiting the occupancy to under 20\%. However, both the DG and SUPG kernels are able to utilize a large fraction of the device throughput -- memory throughput in the DG case and compute throughput in the SUPG case.

To further understand the difference in performance between the SUPG and DG schemes, we used the Nsight Compute profiler to obtain the \textit{arithmetic intensity} for both Jacobian assembly kernels, which is the ratio of FLOPS performed to bytes of memory accessed. The arithmetic intensity is 0.22 FLOPS/byte for the SUPG case and 0.1 FLOPS/byte for the DG case, which partly explains why SUPG achieves higher compute throughput than DG. 

Ultimately, the deciding factor in the more suitable method for GPU offloading is the speedup factor relative to the CPU. According to this criterion, the SUPG scheme is better in an implicit time stepping context where matrix assembly is required. However, for an explicit time stepping method, only vector assembly is required, and therefore the DG scheme will perform better on the GPU.
\begin{table}[t]
    \centering
\begin{tabular}{llrrrr}
\toprule
Method & Kernel & \begin{tabular}{@{}l}Theoretical\\ occupancy (\%) \end{tabular} & \begin{tabular}{@{}l}Achieved\\ occupancy (\%) \end{tabular} & \begin{tabular}{@{}l}Memory\\ throughput (\%) \end{tabular} & \begin{tabular}{@{}l}Compute\\ throughput (\%) \end{tabular} \\
\midrule
DG   & Jacobian & 12.50 & 12.33 & 41.44 & 10.43 \\
DG   & Residual & 18.75 & 18.74 & 64.97 & 18.20 \\
SUPG & Jacobian & 12.50 & 12.33 & 48.66 & 59.31 \\
SUPG & Residual & 12.50 & 12.39 &  5.64 & 77.85 \\
\bottomrule
\end{tabular}
\caption{Profiling statistics for the GPU assembly kernels on a square mesh}
    \label{tab:tidal_prof}
\end{table}
\subsection
{Navier--Stokes Equation}

Although matrix assembly is a crucial part of the finite element method, most solvers also require the solution of linear systems. To assess the practicality of offloading typical FEniCSx code to the GPU, we modify an existing FEniCSx Navier--Stokes solver~\citep{dokkenipcs} to use cuDOLFINx.
The modified solver is hosted in a forked GitHub repository \citep{pachevipcs}.
The solver uses an incremental pressure correction scheme~\citep{timmermans1996approximate,simo1994unconditional} that requires three stages per time step.
Although each stage requires a linear solve and assembly of a vector, only the first requires reassembly of a stiffness matrix. Second-order Lagrange tetrahedral elements are used for the velocity field and first-order Lagrange elements for the pressure field.
The mesh is a refined version of the 3D channel with an obstacle used in the original code and contains 1,995,628 tetrahedra with 355,319 vertices.

The same hardware is used for comparison as in the Poisson experiment, namely, the Hopper GPU and Grace CPU within a GH200 Superchip.
\Cref{tab:navier_stokes_results} provides the average timing results over 100 time steps for each component of the solver.
\begin{table}[t]
    \centering
\begin{tabular}{clrrrr}
\toprule
      &                 & \multicolumn{2}{c}{Hopper (GPU)} & \multicolumn{2}{c}{Grace (CPU)} \\
                          \cmidrule(lr){3-4}               \cmidrule(lr){5-6}
Stage & Solver/preconditioner       & Assembly & Solve               & Assembly & Solve \\
\midrule
    1 & BCGS/Jacobi     &    0.348 &               1.601 &    0.906 & 6.339 \\
    2 & GMRES/BoomerAMG &    0.009 &               0.013 &    0.007 & 0.108 \\
    3 & CG/Jacobi       &    0.004 &               0.079 &    0.013 & 0.564 \\
\bottomrule
\end{tabular}
\caption{Runtime (in seconds) for each stage of the Navier--Stokes solver}
    \label{tab:navier_stokes_results}
\end{table}
Overall, the CUDA-accelerated solver averaged 2.07\,s per time step, while the original solver took 7.89\,s per time step. The overall speedup is therefore a factor of three.8${\times}$. While the solution time is dominated by the linear solve for the first stage, assembly comprises over 10\% of the total runtime for both the GPU and CPU codes. 

We note that, compared to the Poisson example, the assembly speedup for the Navier--Stokes code is lower. We hypothesize this is due to the use of second-order elements for the velocity field. Higher-order elements are known to pose difficulties for the GPU offloading approach used within cuDOLFINx, because the auto-generated GPU kernels can require a large number of registers. This results in a phenomenon known as \textit{register spilling}, in which the GPU kernels are forced to utilize global memory to store some local variables, which degrades performance. Solutions include reworking the generated code to better hide memory transfer latency, potentially by reducing register spillage or minimizing the use of local memory. Another approach is to assemble each element using multiple GPU threads to reduce the required resources per thread. Both of these solutions would require substantial enhancements to FFCx, the form compiler within the FEniCS project. Nevertheless, the speedup obtained for assembly with quadratic elements in this case is still significant.
\section
{Conclusion}
We have introduced cuDOLFINx, an add-on enhancement to FEniCSx that provides GPU-accelerated assembly routines. We have demonstrated that offloading assembly to a GPU can provide significant speedup compared to multi-core CPUs, particularly for first-order elements. The package has a simple Python interface and can be utilized in FEniCSx codebases with little effort.

In the near future, we intend to add support for multiple GPUs. Subsequent efforts will focus on providing support for non-NVIDIA GPUs, as well as developing efficient kernels for assembly with high-order elements. Work is currently underway to expand the documentation and examples, as well as to create easily installable binary distributions.
\section
{Supplementary Material}
The source code of cuDOLFINx is archived at \citep{cudolfinxzenodo}.

\begin{acknowledgement}
  We gratefully acknowledge the use of the Lonestar6 and Vista systems at the Texas Advanced Computing Center under the ADCIRC allocation.
  The research presented in this paper has benefited from the Experimental Infrastructure for Exploration of Exascale Computing (eX3), which is financially supported by the Research Council of Norway under contract 270053.
\end{acknowledgement}

\bibliographystyle{spbasic}
\bibliography{chapters/pachev/bibliography.bib}